{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W35fXE7QhQHn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from awsglue.dynamicframe import DynamicFrame\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(\"Starting Glue Job: Script 4 - CURATED Parquet -> Redshift (Create Table + Load)\")\n",
        "print(\"Note: Redshift connection is attached in Glue Job UI (no connectionName param used)\")\n",
        "\n",
        "# Read job parameters passed from Glue Console\n",
        "args = getResolvedOptions(sys.argv, [\n",
        "    \"JOB_NAME\",\n",
        "    \"CURATED_PATH\",\n",
        "    \"REDSHIFT_DB\",\n",
        "    \"REDSHIFT_SCHEMA\",\n",
        "    \"REDSHIFT_TABLE\",\n",
        "    \"REDSHIFT_TMP_DIR\"\n",
        "])\n",
        "\n",
        "# Initialize Spark and Glue contexts\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "# Initialize Glue Job\n",
        "job = Job(glueContext)\n",
        "job.init(args[\"JOB_NAME\"], args)\n",
        "\n",
        "curated_path = args[\"CURATED_PATH\"]          # S3 CURATED layer path (partitioned)\n",
        "redshift_db = args[\"REDSHIFT_DB\"]            # Redshift database name (example: dev)\n",
        "redshift_schema = args[\"REDSHIFT_SCHEMA\"]    # Redshift schema (example: public)\n",
        "redshift_table = args[\"REDSHIFT_TABLE\"]      # Redshift table name (example: curated_users)\n",
        "redshift_tmp_dir = args[\"REDSHIFT_TMP_DIR\"]  # S3 temp dir required by Glue Redshift writer\n",
        "\n",
        "users_table = f\"{redshift_schema}.{redshift_table}\"\n",
        "\n",
        "print(\"Input CURATED Path:\", curated_path)\n",
        "print(\"Target Redshift Database:\", redshift_db)\n",
        "print(\"Target Redshift Table:\", users_table)\n",
        "print(\"Redshift Temp Dir:\", redshift_tmp_dir)\n",
        "\n",
        "# Step 1: Read CURATED Parquet data from S3\n",
        "print(\"Step 1: Reading CURATED Parquet data\")\n",
        "df = spark.read.parquet(curated_path)\n",
        "\n",
        "print(\"Read completed\")\n",
        "print(\"Total records in CURATED:\", df.count())\n",
        "df.printSchema()\n",
        "df.show(5, False)\n",
        "\n",
        "# Step 2: Normalize schema for Redshift\n",
        "print(\"Step 2: Normalizing schema for Redshift\")\n",
        "df_rs = (\n",
        "    df\n",
        "    .withColumn(\"signup_date\", F.col(\"signup_date\").cast(\"date\"))\n",
        "    .withColumn(\"is_active\", F.col(\"is_active\").cast(\"boolean\"))\n",
        "    .select(\"user_id\", \"username\", \"email\", \"city\", \"signup_date\", \"is_active\")\n",
        ")\n",
        "\n",
        "print(\"Normalized schema:\")\n",
        "df_rs.printSchema()\n",
        "df_rs.show(5, False)\n",
        "\n",
        "# Convert DataFrame to DynamicFrame (required for Glue Redshift writer)\n",
        "print(\"Step 3: Converting DataFrame to DynamicFrame\")\n",
        "dyf = DynamicFrame.fromDF(df_rs, glueContext, \"dyf_redshift\")\n",
        "print(\"DynamicFrame conversion completed\")\n",
        "\n",
        "# Step 4: Create table in Redshift (preactions)\n",
        "print(\"Step 4: Preparing Redshift CREATE TABLE statement\")\n",
        "\n",
        "create_sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {users_table} (\n",
        "  user_id BIGINT,\n",
        "  username VARCHAR(100),\n",
        "  email VARCHAR(150),\n",
        "  city VARCHAR(100),\n",
        "  signup_date DATE,\n",
        "  is_active BOOLEAN\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "print(\"CREATE TABLE SQL:\")\n",
        "print(create_sql)\n",
        "\n",
        "# Step 5: Load data into Redshift\n",
        "print(\"Step 5: Writing data into Redshift (using Glue Connection via from_jdbc_conf)\")\n",
        "\n",
        "glueContext.write_dynamic_frame.from_jdbc_conf(\n",
        "    frame=dyf,\n",
        "    catalog_connection=\"Redshift connection\",   # must match Glue Connection name exactly\n",
        "    connection_options={\n",
        "        \"database\": redshift_db,\n",
        "        \"dbtable\": users_table,\n",
        "        \"preactions\": create_sql,\n",
        "        \"redshiftTmpDir\": redshift_tmp_dir\n",
        "    },\n",
        "    transformation_ctx=\"redshift_write\"\n",
        ")\n",
        "\n",
        "print(\"Redshift write completed\")\n",
        "\n",
        "# Commit the Glue job\n",
        "job.commit()\n",
        "print(\"Glue Job Completed: Script 4\")"
      ]
    }
  ]
}