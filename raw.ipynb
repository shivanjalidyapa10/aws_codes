{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W35fXE7QhQHn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(\"Starting Glue Job: Script 1 - RAW CSV to PROCESSED Parquet\")\n",
        "\n",
        "# Read job parameters passed from Glue Console\n",
        "args = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"SRC_PATH\", \"PROCESSED_PATH\"])\n",
        "\n",
        "# Initialize Spark and Glue contexts\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "# Initialize Glue Job\n",
        "job = Job(glueContext)\n",
        "job.init(args[\"JOB_NAME\"], args)\n",
        "\n",
        "# Input path: RAW layer CSV location in S3\n",
        "src = args[\"SRC_PATH\"]\n",
        "\n",
        "# Output path: PROCESSED layer Parquet location in S3\n",
        "processed = args[\"PROCESSED_PATH\"]\n",
        "\n",
        "print(\"Input RAW Path:\", src)\n",
        "print(\"Output PROCESSED Path:\", processed)\n",
        "\n",
        "# Step 1: Read CSV data from S3 RAW layer\n",
        "print(\"Step 1: Reading CSV file from RAW layer\")\n",
        "\n",
        "df = (\n",
        "    spark.read\n",
        "    .option(\"header\", \"true\")          # First row contains column names\n",
        "    .option(\"inferSchema\", \"true\")     # Automatically detect column data types\n",
        "    .csv(src)                         # Load CSV file from S3 path\n",
        ")\n",
        "\n",
        "print(\"CSV Read Completed Successfully\")\n",
        "print(\"Total Records in RAW file:\", df.count())\n",
        "\n",
        "# Step 2: Apply transformations and standardize the dataset\n",
        "print(\"Step 2: Applying transformations\")\n",
        "\n",
        "df_t = (\n",
        "    df\n",
        "\n",
        "    # Convert user_id from string to integer for consistency\n",
        "    .withColumn(\"user_id\", F.col(\"user_id\").cast(\"int\"))\n",
        "\n",
        "    # Remove leading/trailing spaces from username\n",
        "    .withColumn(\"username\", F.trim(F.col(\"username\")))\n",
        "\n",
        "    # Convert email to lowercase and remove extra spaces\n",
        "    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\"))))\n",
        "\n",
        "    # Standardize city names (example: kansas city â†’ Kansas City)\n",
        "    .withColumn(\"city\", F.initcap(F.trim(F.col(\"city\"))))\n",
        "\n",
        "    # Convert signup_date column into proper date format\n",
        "    .withColumn(\"signup_date\", F.to_date(F.col(\"signup_date\"), \"M/d/yy\"))\n",
        "\n",
        "\n",
        "    # Convert is_active column into boolean type\n",
        "    .withColumn(\"is_active\", F.col(\"is_active\").cast(\"boolean\"))\n",
        "\n",
        "    # Add ingestion timestamp to track when the record was processed\n",
        "    .withColumn(\"ingest_ts\", F.current_timestamp())\n",
        ")\n",
        "\n",
        "print(\"Transformations Completed Successfully\")\n",
        "\n",
        "# Step 3: Remove duplicate records based on user_id\n",
        "print(\"Step 3: Removing duplicates based on user_id\")\n",
        "\n",
        "df_t = df_t.dropDuplicates([\"user_id\"])\n",
        "\n",
        "print(\"Total Records After Deduplication:\", df_t.count())\n",
        "\n",
        "# Step 4: Write cleaned output as Parquet into PROCESSED layer\n",
        "print(\"Step 4: Writing output to PROCESSED layer in Parquet format\")\n",
        "\n",
        "df_t.write \\\n",
        "    .mode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .save(processed)\n",
        "\n",
        "print(\"Parquet Write Completed Successfully\")\n",
        "\n",
        "# Commit the Glue job\n",
        "job.commit()\n",
        "\n",
        "print(\"Glue Job Completed Successfully\")"
      ]
    }
  ]
}